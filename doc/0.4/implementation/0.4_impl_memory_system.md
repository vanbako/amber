# Amber 0.4 Memory System Implementation

This document covers the load/store pipeline, cache hierarchy, and capability metadata plumbing required by Amber v0.4. It assumes the module layout called out in `0.4_impl_overview.md` and cross-links with the core pipeline and multicore fabric notes.

## Architectural Targets

- Capability-aware memory accesses must observe tag, bounds, seal, and permission metadata without stalling unrelated micro-ops.
- The L1 data cache is non-blocking with at least four MSHRs per core, supports 128-bit atomics, and respects the LOAN/TRACE permission bits introduced in v0.4.
- Dual-core configurations share a 256 KiB victim/tag slice that behaves inclusively and guarantees visibility for capability tag updates.

## Module Topology

| Component | Planned Module | Highlights |
| --- | --- | --- |
| Load/Store Queue | `rtl/mem/lsu/lsq.sv` | Tracks 24 in-flight entries, capability metadata, ordering fences. |
| Capability Metadata Cache | `rtl/mem/tag_store/cap_tag_cache.sv` | Mirrors capability tags + bounds per cache line; interfaces with LSQ and L1. |
| Data Cache | `rtl/mem/dcache/dcache_top.sv` | Non-blocking, dual-ported for load vs. store, integrates tag cache. |
| Victim/Tag Slice | `rtl/mem/victim/victim_tag_slice.sv` | Shared inclusive slice with capability metadata and snoop filter. |
| Atomic Unit | `rtl/mem/atomic/atomic_unit.sv` | Handles 128-bit LR/SC, set/clear loan bits atomically. |
| Memory Arbiter | `rtl/mem/fabric/mem_arbiter.sv` | Bridges core-side AXI-like requests into the shared fabric. |

## Load/Store Queue (LSQ)

- Maintain separate head/tail indices for loads and stores to allow disjoint retirement. Each entry stores:
  - Physical data address and capability cursor (post-normalisation).
  - Pointer to capability metadata (tag cache index, bounds snapshot, permission bits).
  - Ordering attributes (fence, release/acquire) and loan actions.
- Loads may bypass earlier loads but must wait for older stores with unresolved addresses or capability metadata. Use a CAM to detect address overlap and a lightweight scoreboard for capability overlaps.
- Stores only commit once the ROB confirms retirement **and** the victim/tag slice acknowledges the tag update. Until then the capability metadata remains speculative.

## Capability Metadata Handling

- The tag cache mirrors per-line capability data. For each 64-byte cache line, store:
  - Execute/load/store permission masks.
  - Seal/loan bits.
  - Bounds compressed representation (base pointer + length).
- LSQ entries consult the tag cache before issuing to memory. On a miss, the request proceeds to the victim/tag slice; the core must stall only the affected micro-op, not the entire pipeline.
- Tag updates triggered by stores propagate to the victim slice via a sideband channel. Ensure the channel carries enough context (core ID, line address, metadata payload) for coherence replay.

## Data Cache and MSHRs

- The default configuration instantiates a 32 KiB, four-way associative cache with 64-byte lines. Parameterise size and associativity for experimentation.
- At least four outstanding misses (MSHRs) per core are required to hide DRAM latency. Each MSHR tracks both data refill and capability metadata refill events so that loads only resume when both arrive.
- Implement separate pipelines for load hits, store hits, and miss handling to meet the 180 MHz timing target on Arora-V.
- Integrate a write combining buffer for capability tag writes so consecutive updates do not thrash the victim slice.

## Victim / Tag Slice

- Lives outside the core; instantiated once per dual-core cluster. The slice acts as:
  - An inclusive victim cache for both data and capability tags.
  - The point of coherence for capability loans and trace permissions.
- Provide snoop responses that invalidate or downgrade L1 lines when another core updates a capability tag. Tag-only updates should not force data eviction unless required by bounds shrink.
- Expose debug counters (per-core hit/miss, tag-only transactions) for bring-up; hook them into the shared CSR fabric.

## Atomic and Loan Operations

- Implement 128-bit LR/SC as a dedicated sub-unit that observes capability tags via the LSQ metadata. Ensure that failed SCs re-validate tags before reporting failure.
- Loan and trace permissions modify metadata without altering the raw pointer value. Encode them as part of the tag cache entry so they survive eviction/reload.
- Provide fences (`cperm.sync`, etc.) that flush LSQ entries whose outstanding metadata updates could violate visibility.

## Interaction with the Core Pipeline

- The core issues memory micro-ops via a request channel that pairs with the LSQ. Backpressure from the LSQ should throttle dispatch but leave other clusters unaffected.
- Capability faults detected in the LSQ or tag cache must propagate back to the ROB with full context (faulting capability ID, offending permission, address).
- Speculative loads mark targeted capability registers as "speculative" until commit. If a mispredict flush occurs, the LSQ sends cancellation events so the tag cache discards speculative updates.

## Outstanding Tasks

- Define the binary format of tag cache lines. Options include packed structs or dual memories (data vs. capability).
- Decide whether the victim/tag slice speaks AXI, TileLink, or a custom protocol. The coherence document will lock this down; implementers should plan for adapter modules.
- Evaluate the resource cost of supporting eight vs. four MSHRs on the target FPGA and capture results in this document once measured.
