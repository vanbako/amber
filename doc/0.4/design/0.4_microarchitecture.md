# Amber 0.4 Microarchitecture Overview

Amber v0.4 keeps the v0.3 capability-aware ISA and system model while introducing a deeper, speculative pipeline to close the IPC gap with contemporary embedded CPUs. The goal is to preserve software compatibility and toolchain continuity while enabling higher clocks, wider issue, and better latency hiding for capability-heavy workloads while mapping cleanly onto the Arora-V 138K FPGA in single- or dual-core form.

## Design Objectives

- Maintain full binary compatibility with v0.3, including the 48-bit bundle fetch format, GC/SC register files, and capability semantics.
- Deliver a 2x speedup on mixed integer + capability code by widening fetch/decode and enabling out-of-order issue with low penalty for capability checks.
- Integrate modern branch prediction, non-blocking caches, and a memory hierarchy that respects capability tagging without stalling the core.
- Keep the microarchitecture scalable from a compact two-cluster implementation to higher-end four-cluster derivatives by parameterizing queue and cache sizes.
- Fit up to two cores and their shared infrastructure within the Arora-V 138K LUT/BRAM budget at a 180 MHz nominal clock (200 MHz stretch goal) without relaxing capability guarantees.

## Pipeline Topology

Amber v0.4 adopts an 11-stage, out-of-order pipeline organized into front-end, rename/scheduling, execution, and commit regions:

1. **IF0** - Branch predictor lookup (global history, BTB, RAS, capability-aware I-TLB).
2. **IF1** - Bundle fetch (up to two 48-bit bundles per cycle from the I-cache).
3. **IF2** - Bundle align & predecode (slot enumeration, capability prefix detection).
4. **DEC** - Decode + micro-op crack (up to four micro-ops per cycle).
5. **REN** - Integer + capability register rename (virtual queues assigned).
6. **DISP** - Dispatch into clustered issue queues and ROB allocation.
7. **ISS** - Issue / wakeup-select within integer, capability, load/store, and async units.
8. **EX** - Execution pipelines (1-cycle ALU, 3-cycle capability norm, 3/5-cycle mul/div).
9. **MEM** - Load/store unit with capability window check plus TLB/tag verify.
10. **WB** - Writeback to physical registers; LSQ completes stores after tag commit.
11. **RET** - Reorder buffer commit plus architectural capability tag reconciliation.

Stages 5 through 10 are decoupled by queues so that front-end hiccups or long capability operations do not globally stall the core. For FPGA deployment, the stage boundaries align with block RAM and DSP placement to minimise long routing detours.

## Front-End Enhancements

- **Predictor:** Hybrid TAGE plus loop predictor with a 4K-entry BTB and 16-entry return stack. Capability-sealed targets are cached with their required execute permissions, allowing the predictor to skip waiting on capability metadata when possible.
- **Fetch:** Two 128-bit fetch blocks feed the bundle aligner, allowing up to eight 12-bit instructions or four 24-bit instructions to enter decode per cycle. On Arora-V, the fetch path maps to dual BRAM ports with a single-cycle retiming stage to meet the 180 MHz goal.
- **I-TLB & Tags:** Capability tags are cached alongside I-cache lines. Fetched bundles carry their execute permission summary so speculative decode can continue while tag validation completes in IF2.

## Rename and Scheduling

- **Integer Rename:** 64 physical integer registers back the 16 architectural GPRs, supporting four-wide rename per cycle. FPGA builds may trim to 48 physical registers for the second core if timing pressure appears, at the cost of slightly more rename stalls.
- **Capability Rename:** 24 physical capability registers cover the 18 architectural GC/SC slots. Capability physical registers track tag state, bounds metadata, and seal bits; each entry includes a pointer to the last authority-reduction operation for quick rollback.
- **ROB:** 160-entry reorder buffer sized to cover the deeper pipeline and latency of capability checks. Each entry logs the source capability physical IDs to reinstate tags and bounds on mispredict recovery. For two-core builds, each core instantiates an independent ROB; the shared debug fabric - the on-FPGA trace and CSR capture network - samples head/tail pointers and commit causes to maintain observability without cross-core contention or in-flight-state sharing.
- **Issue Queues:** Separate queues for ALU (24 entries), capability operations (16 entries), load/store (24 entries), and asynchronous engines (8 entries). The capability queue enforces tag/permission dependency order by waiting for the dedicated capability check unit to broadcast readiness.

## Capability Execution Flow

- **Check Unit:** Capability operations (load/store, seal, set bounds, jumps) issue through a two-stage normalize/check pipeline. Stage EX normalizes cursor and bounds; stage MEM performs tag and permission verification against the capability metadata and pending LSQ operations.
- **Speculation Safety:** Speculative capability dereferences mark the associated physical capability entry as "speculative-used." If the instruction is squashed, commit logic restores the previous tag state to prevent hijacked speculation from forging authority.
- **Loan/Trace Metadata:** LOAN and TRACE permission bits latch with the capability ID inside the issue queue so loads, stores, and capability moves propagate policy into the LSQ. Directory responses carry the bits back to the core, letting the commit stage flag expired loans before data leaves MEM.
- **Sealed Control Transfers:** <code>capability_jump</code> and <code>capability_return</code> reserve slots in the ROB to ensure their tag and seal checks retire in order. The pipeline allows them to issue early but prevents commit until all preceding instructions reach RET.

## Memory System

- **L1 Data Cache:** 32 KB, 4-way, write-back with 16-byte sectors holding a one-bit tag per 16 bytes. Tag bits flow through the LSQ to keep capability metadata coherent with speculation. FPGA builds bank the cache across BRAM blocks with registered outputs to avoid Fmax cliffs.
- **LSQ:** 32-entry load queue plus 24-entry store queue with capability pointers. Loads may bypass stores when bounds-overlap checks pass; otherwise they wait for store resolution or trigger a <code>BOUNDS_WAIT</code> replay.
- **MMU:** Shared 48-entry fully-associative D-TLB with permission mirroring for capability bits. Capability-enforced accesses consult both TLB permissions and capability metadata.
- **Non-blocking:** Miss-status holding registers (MSHRs) allow up to eight outstanding cache misses per core in the base configuration; capability tags, TRACE, and LOAN summaries are returned with the data beat and validated before ROB commit. The Arora-V dual-core build overrides this to an 8/6 entry split (core0/core1) via the <code>mshr_per_core</code> synthesis parameter to keep congestion manageable.

- **Coherence Protocol (Amber Coherence Fabric):** Private L1s implement the {I, S, E, M} state machine defined in the platform architecture spec. Lines fill into `S` on read-only demand; capability stores, 128-bit atomics, or prefix windows that request write permission upgrade via `ReqRdx` to `E/M`. The shared 256 KB slice is the home agent: each directory entry tracks a 4-bit sharer vector plus the R/W/X/TRACE/LOAN summary, and it issues `SnpInv` or `SnpClean` probes so L1s reconcile capability metadata before data is observed. External masters attach through the ACF request/response channels; transactions marked non-coherent map to the fabric's `NC` type so the core bypasses the slice and relies on `capability_fence prefix` and `DMACTL.sync_dma` fences, matching the system-level guarantees.

## Branch Recovery and Exceptions

- **Recovery:** Global history, rename maps, and capability tag shadow state snapshot on every predicted-taken branch. The branch mispredict penalty is 11 cycles (full pipeline flush) but averages 5 cycles for same-line targets due to dual-bundle fetch and warmed predictor tables.
- **Precise Faults:** Capability faults raise traps at commit; the ROB drains younger instructions while retaining the offending capability metadata for CSR logging.
- **Interrupts:** Precise interrupt delivery occurs at ROB boundaries. The front-end inserts a synthetic micro-op that saves the current <code>PCc</code> and <code>PSTATEc</code> into <code>SC1</code> and <code>SC4</code> before vectoring via <code>SC0</code>.

## Asynchronous Units and Dataflow

- Multiply/divide, floating-point, and Oracle-NUMBER units sit behind the asynchronous issue queue. They accept tagged capability descriptors for operand buffers, allowing in-flight data sharing without extra copies.
- CSR interactions complete through the ROB to maintain ordering with memory and capability updates.

## Arora-V 138K Dual-Core Implementation

- **Frequency and floorplanning:** Target 180 MHz nominal with 200 MHz stretch by inserting retiming flops at BRAM/DSP boundaries and using quadrant-local routing per core. Floorplan places each core in an opposite Arora-V quadrant with the shared fabric centred to minimise inter-core trace length.
- **Resource budgeting:** Allocate roughly 60k LUTs and 120 BRAMs per core, leaving headroom for shared L2, interconnect, and debug. Capability rename structures keep the first core in distributed RAM so the rename bypass network stays inside the LUT fabric (short, fast nets), while the second core mirrors the tables into block RAMs to trim LUT usage once inter-core routing dominates placement.
- **Shared memory hierarchy:** Each core retains private 32 KB L1s. A shared 256 KB victim/tagged-cache slice acts as the common L2: it is 4-way, 2-cycle hit latency, built from dual-ported BRAM banks behind a round-robin arbiter. The slice records capability permission summaries alongside the line state so either core can satisfy an access without re-running full bounds checks, while L1 evictions fall back into it as a victim cache. The slice exports the Amber Coherence Fabric (ACF) home port, translating core `ReqRd/ReqRdx` traffic into the platform-level `Req/Resp/Snp` message set and driving `SnpInv` probes when capability permissions tighten or LOAN timers expire so the system fabric sees a single point of serialization.
- **Inter-core coherence:** A lightweight directory tracks line ownership with 4-bit sharer vectors. Capability metadata moves with the directory entries, ensuring that cross-core handoffs never expose stale bounds or permissions.
- **Clocking and power:** Clock domains stay unified to avoid CDC complexity; fine-grained clock gating on the async unit queue, predictor tables, and LSQ halves toggling when workloads are single-threaded. Voltage scaling is deferred, but power islands are annotated for future silicon spin.
- **Toolchain hooks:** Synthesis scripts expose parameters for physical register count, queue depth, cache size, and <code>mshr_per_core</code>. For dual-core Arora-V builds, the default configuration trims the second core's asynchronous issue queue to six entries and programs <code>mshr_per_core</code> to {8,6} (core0/core1) to reduce routing congestion while keeping capability latency low; clearing the override restores the symmetric eight-entry setting.

## Compatibility and Migration

- v0.3 binaries run unchanged; the assembler and linker emit identical bundles. New performance comes from hardware only.
- v0.4 formalizes the deferred 128-bit LR/SC by leveraging the wider LSQ and capability check unit. Toolchain updates will expose the ISA once RTL support is validated.
- Debug and profiling hooks expose per-cluster occupancy, capability queue pressure, and tag replay counts to help software tune memory layouts.

## Open Items for v0.4 Design Freeze

- Finalize physical register counts for high-end SKUs (option to expand to 96 integer and 32 capability entries) and confirm dual-core Arora-V closure at 200 MHz.
- Complete the DV plan for speculative capability rollback paths, LSQ tag forwarding, and directory-based coherence interactions between cores.
- Determine power and performance trade-offs for optional dual-issue capability check units in multi-cluster configurations.

